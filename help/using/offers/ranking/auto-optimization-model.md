---
product: experience platform
solution: Experience Platform
title: Automatiskt optimerade modeller
description: L√§s mer om modeller f√∂r automatisk optimering
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: a85de6a9-ece2-43da-8789-e4f8b0e4a0e7
source-git-commit: 25b1e6050e0cec3ae166532f47626d99ed68fe80
workflow-type: tm+mt
source-wordcount: '1358'
ht-degree: 0%

---

# Automatiskt optimerade modeller {#auto-optimization-model}

En automatisk optimeringsmodell syftar till att leverera erbjudanden som maximerar den avkastning (KPI) som aff√§rsklienterna s√§tter. Dessa nyckeltal kan vara i form av konverteringsgrader, int√§kter osv. I nul√§get fokuserar automatisk optimering p√• att optimera erbjudandeklick med erbjudandekonvertering som m√•l. Automatisk optimering √§r icke-personaliserat och optimerar baserat p√• erbjudandets&quot;globala&quot; prestanda.

## Begr√§nsningar {#limitations}

Anv√§ndningen av automatiska optimeringsmodeller f√∂r beslutshantering omfattas av nedanst√•ende begr√§nsningar:

* Automatiska optimeringsmodeller fungerar inte med API:t f√∂r gruppbeslut.
* Feedback som beh√∂vs f√∂r att skapa en modell m√•ste skickas in som en upplevelseh√§ndelse. Den ska inte skickas in automatiskt i [!DNL Journey Optimizer] kanaler.

## Terminologi {#terminology}

F√∂ljande termer √§r anv√§ndbara n√§r du diskuterar automatisk optimering:

* **Flerarmad bandit**: En [flerarmad bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit){target="_blank"}-metod f√∂r optimering balanserar unders√∂kande inl√§rning och utnyttjande av inl√§rningen.

* **Thomson-sampling**: Thompson-sampling √§r en algoritm f√∂r onlinebeslutsproblem d√§r √•tg√§rder vidtas sekventiellt p√• ett s√§tt som m√•ste balansera mellan att utnyttja det som √§r k√§nt f√∂r att maximera omedelbara prestanda och investera f√∂r att samla in ny information som kan f√∂rb√§ttra framtida prestanda. [L√§s mer](#thompson-sampling)

* [**Beta-distribution**](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}: En upps√§ttning kontinuerliga [sannolikhetsf√∂rdelningar](https://en.wikipedia.org/wiki/Probability_distribution){target="_blank"} som definieras i intervallet [0, 1] [parametriserad](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} av tv√• positiva [formparametrar](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"}.

## Thompson Sampling {#thompson-sampling}

Den algoritm som ligger till grund f√∂r automatisk optimering √§r **Thompson sampling**. I det h√§r avsnittet diskuterar vi intuitionen bakom Thompson-provtagning.

[Thompson sampling](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, eller Bayesian bandits, √§r en bayesisk l√∂sning p√• problemet med flerarmad bandit.  Grundtanken √§r att behandla den genomsnittliga bel√∂ningen ùõç fr√•n varje erbjudande som en **slumpm√§ssig variabel** och anv√§nda de data som vi har samlat in hittills f√∂r att uppdatera v√•r&quot;tro&quot; om den genomsnittliga bel√∂ningen. Denna trosuppfattning representeras matematiskt av en **sannolikhetsf√∂rdelning efter** - i stort sett ett intervall av v√§rden f√∂r den genomsnittliga bel√∂ningen, tillsammans med den sannolikhet (eller sannolikhet) som bel√∂ningen har f√∂r varje erbjudande.‚ÄØF√∂r varje beslut ska vi sedan **ta ett prov fr√•n var och en av dessa bel√∂ningsf√∂rdelningar** och v√§lja det erbjudande vars provbel√∂ning hade det h√∂gsta v√§rdet.

Denna process illustreras i bilden nedan, d√§r vi har tre olika erbjudanden. Till att b√∂rja med har vi inga bevis fr√•n data och vi antar att alla erbjudanden har en enhetlig f√∂rdelning efter bel√∂ningen. Vi tar ett prov fr√•n varje offerts f√∂rdelning efter bel√∂ningen. Det exempel som valts ut fr√•n distributionen av erbjudandet 2 har det h√∂gsta v√§rdet. Detta √§r ett exempel p√• **utforskande**. N√§r vi har visat erbjudandet 2 samlar vi in eventuell bel√∂ning (t.ex. konvertering/ingen konvertering) och uppdaterar posteriordistributionen av erbjudandet 2 med hj√§lp av Bayes Theorem enligt nedan.  Vi forts√§tter med den h√§r processen och uppdaterar efterhandsf√∂rdelningen varje g√•ng ett erbjudande visas och bel√∂ningen samlas in. I den andra siffran v√§ljs erbjudande 3 - trots att erbjudandet 1 har den h√∂gsta genomsnittliga bel√∂ningen (den posteriorbel√∂ningsf√∂rdelningen ligger l√§ngst till h√∂ger) har provtagningsprocessen fr√•n varje distribution lett till att vi valt ett till synes ooptimalt erbjudande 3. P√• s√• s√§tt ger vi oss sj√§lva m√∂jlighet att l√§ra oss mer om den verkliga bel√∂ningsf√∂rdelningen i Erbjudande 3.

I takt med att fler prover samlas in √∂kar f√∂rtroendet och en mer korrekt uppskattning av den m√∂jliga bel√∂ningen g√∂rs (motsvarande mindre bel√∂ningar).‚ÄØDen h√§r processen med att uppdatera v√•r tro allt eftersom fler bevis blir tillg√§ngliga kallas **Bayesian Inference**.

Om ett erbjudande (t.ex. erbjudande 1) √§r en tydlig vinnare kommer dess bel√∂ningsf√∂rdelning att separeras fr√•n andra. F√∂r varje beslut √§r den utvalda bel√∂ningen fr√•n erbjudande 1 troligtvis den h√∂gsta, och vi v√§ljer den med st√∂rre sannolikhet. Det h√§r √§r **utnyttjande** - vi √§r mycket √∂vertygade om att erbjudande 1 √§r det b√§sta, s√• det v√§ljs f√∂r att maximera bel√∂ningar.

![](../assets/ai-ranking-thompson-sampling.png)

**Figur 1**: *F√∂r varje beslut tar vi ett prov fr√•n en punkt fr√•n bel√∂ningsf√∂rdelningen. Erbjudandet med det h√∂gsta samplingsv√§rdet (konverteringsgrad) kommer att v√§ljas. I den inledande fasen har alla erbjudanden en enhetlig f√∂rdelning eftersom vi inte har n√•gra bel√§gg f√∂r konverteringsgraden f√∂r erbjudandena fr√•n uppgifterna. Efterhandsdistributionen blir sn√§vare och mer exakt n√§r vi samlar in fler prover. I slut√§ndan v√§ljs erbjudandet med den h√∂gsta konverteringsgraden varje g√•ng.*

<!--
![](../assets/ai-ranking-thompson-sampling-initial.png)
![](../assets/ai-ranking-thompson-sampling-intermediate.png)
![](../assets/ai-ranking-thompson-sampling-ultimate.png)
-->

+++**Teknisk information**

F√∂r att ber√§kna/uppdatera distributioner anv√§nder vi **Bayes Theorem**. F√∂r varje erbjudande ***i*** vill vi ber√§kna deras ***P(ùõçi | data)***, dvs. f√∂r varje erbjudande ***i***, hur sannolikt det √§r att bel√∂ningsv√§rdet **ùõçi** √§r, med tanke p√• de data som vi hittills har samlat in f√∂r det erbjudandet.

Fr√•n Bayes Theorem:

***Posterior = Sannolikhet * Tidigare***

Sannolikheten **f√∂r f√∂reg√•ende** √§r den inledande gissningen om sannolikheten f√∂r att skapa utdata. Sannolikheten, efter att vissa bevis har samlats in, kallas f√∂r den **posteriorsannolikheten**.‚ÄØ

Automatisk optimering √§r utformat f√∂r att ta h√§nsyn till bin√§ra bel√∂ningar (klicka/klicka inte). I det h√§r fallet representerar sannolikheten antalet lyckade f√∂rs√∂k fr√•n N-testversioner och modelleras av en **binomialf√∂rdelning**. F√∂r vissa sannolikhetsfunktioner, om du v√§ljer en viss tidigare version, hamnar den bakre delen i samma f√∂rdelning som den f√∂reg√•ende. En s√•dan tidigare version kallas **konjugera tidigare**. Den h√§r typen av f√∂rhandsversioner g√∂r det v√§ldigt enkelt att ber√§kna posteriorf√∂rdelningen. **Beta-f√∂rdelningen** √§r ett konjugat f√∂re binomialsannolikheten (bin√§ra bel√∂ningar) och √§r d√§rf√∂r ett bekv√§mt och vettigt val f√∂r den tidigare och senare sannolikhetsf√∂rdelningen. Beta-distributionen har tv√• parametrar, ***Œ±*** och ***Œ≤***.‚ÄØDessa parametrar kan ses som antalet lyckade och misslyckade samt medelv√§rdet som ges av:

![](../assets/ai-ranking-beta-distribution.png)

Sannolikhetsfunktionen, som vi f√∂rklarade ovan, modelleras av en binomial distribution, med lyckade (konverteringar) och fel (inga konverteringar) och q √§r en [slumpvariabel](https://en.wikipedia.org/wiki/Random_variable){target="_blank"} med en [betadistribution](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}.

F√∂reg√•ende modell baseras p√• distributionen av Beta och den bakre distributionen har f√∂ljande format:

![](../assets/ai-ranking-posterior-distribution.svg)

Bakgrundsv√§rdet ber√§knas genom att antalet lyckade och misslyckade f√∂rs√∂k l√§ggs till i de befintliga parametrarna ***Œ±***, ***Œ≤***.

F√∂r automatisk optimering, som visas i exemplet ovan, b√∂rjar vi med en tidigare distribution av ***Beta(1, 1)*** (enhetlig f√∂rdelning) f√∂r alla erbjudanden och efter att ha lyckats och misslyckats f√∂r ett visst erbjudande, blir den efterst√§llda en Beta-distribution med parametrarna ***(s+Œ±, f+Œ≤)*** f√∂r det erbjudandet.
+++

**Relaterade √§mnen**:

Mer information om Thompson-provtagning finns i f√∂ljande forskningsrapporter:
* [En empirisk utv√§rdering av Thompson Sampling](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analys av Thompson Sampling f√∂r det multiv√§pnade Bandit-problemet](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## Problem med kallstart {#cold-start}

Problemet med&quot;kallstart&quot; uppst√•r n√§r ett nytt erbjudande l√§ggs till i en kampanj och det inte finns n√•gra tillg√§ngliga uppgifter om det nya erbjudandets konverteringsgrad. Under den h√§r perioden m√•ste vi ta fram en strategi f√∂r hur ofta det nya erbjudandet v√§ljs s√• att prestandas√§nkningen minimeras, samtidigt som vi samlar in information om konverteringsgraden f√∂r det nya erbjudandet. Det finns flera l√∂sningar f√∂r att ta itu med det h√§r problemet. Nyckeln √§r att hitta en balans mellan utforskandet av detta nya erbjudande, samtidigt som vi inte offrar utnyttjandet s√• mycket. F√∂r n√§rvarande anv√§nder vi&quot;enhetlig f√∂rdelning&quot; som v√•r f√∂rsta gissning om det nya erbjudandets konverteringsgrad (tidigare distribution). I princip ger vi alla konverteringsgrader samma sannolikhet f√∂r f√∂rekomst.


![](../assets/ai-ranking-cold-start-strategies.png)

**Figur 2**: *√ñverv√§g en kampanj med 3 erbjudanden. Medan kampanjen √§r aktiv l√§ggs erbjudande 4 till i kampanjen. Till att b√∂rja med har vi inga uppgifter om konverteringsgraden f√∂r erbjudande 4 och vi m√•ste ta itu med problemet med kallstart. Vi anv√§nder enhetlig distribution som v√•r f√∂rsta gissning om konverteringsgraden i Erbjudande 4, medan vi samlar in data f√∂r det nya erbjudandet. Som f√∂rklaras i avsnittet [Thompson sampling](#thompson-sampling) kan vi ta prov p√• punkter fr√•n offerternas efterkommande bel√∂ningar och v√§lja det erbjudande som har det h√∂gsta exempelv√§rdet f√∂r att v√§lja vilket erbjudande som ska visas f√∂r en anv√§ndare. I exemplet ovan v√§ljs erbjudande 4 och senare baserat p√• den bel√∂ning som samlats in, uppdateras den bakre f√∂rdelningen av erbjudandet enligt beskrivningen i avsnittet [Thompson sampling](#thompson-sampling).*

## Lyft m√§tning {#lift}

&quot;Lyft&quot; √§r det m√§tv√§rde som anv√§nds f√∂r att m√§ta resultatet f√∂r alla strategier som anv√§nds i rangordningstj√§nsten, j√§mf√∂rt med baslinjestrategin (serbjudandena √§r bara slumpm√§ssiga).

Om vi t.ex. √§r intresserade av att m√§ta resultatet f√∂r en Thompson Sampling-strategi (TS) som anv√§nds i rangordningstj√§nsten och KPI √§r konverteringsgraden (CVR), definieras&quot;lyften&quot; i TS-strategin mot baslinjestrategin som:

![](../assets/ai-ranking-lift.png)
